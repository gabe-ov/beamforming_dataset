{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jupter utilizando puramente a RNN sem a aplicação de uma PCA antes do processo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ler os dados dos csv com os dados simulados contidos nas pastas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ler_csv_e_atribuir_classes_com_pca(diretorio, classe, n_components=5):\n",
        "    dados = []\n",
        "    for arquivo in os.listdir(diretorio):\n",
        "        if arquivo.endswith('.csv'):\n",
        "            caminho_arquivo = os.path.join(diretorio, arquivo)\n",
        "            df = pd.read_csv(caminho_arquivo, sep=';')\n",
        "            features = df.iloc[:, :-1]  \n",
        "            features_normalizadas = (features - features.mean()) / features.std()  # Normalização\n",
        "            df_reduzido = pd.DataFrame(data=features_normalizadas, columns=[f'componente_{i}' for i in range(1, n_components + 1)])\n",
        "            df_reduzido['Classe'] = classe\n",
        "            dados.append(df_reduzido)\n",
        "    return pd.concat(dados, ignore_index=True)\n",
        "\n",
        "diretorio_antes = './64mics/antes'\n",
        "diretorio_depois = './64mics/depois'\n",
        "\n",
        "dados_antes = ler_csv_e_atribuir_classes_com_pca(diretorio_antes, classe=0)\n",
        "\n",
        "dados_depois = ler_csv_e_atribuir_classes_com_pca(diretorio_depois, classe=1)\n",
        "\n",
        "dados_combinados = pd.concat([dados_antes, dados_depois], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11000000, 6)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dados_combinados.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensões das sequências de características: (1100, 10000, 5)\n",
            "Dimensões das sequências de rótulos: (1100,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def split_into_sequences(data, sequence_length=10000):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(0, len(data), sequence_length):\n",
        "        sequence = data.iloc[i:i+sequence_length, :-1]  \n",
        "        label = data.iloc[i+sequence_length-1, -1]  \n",
        "        sequences.append(sequence.values)\n",
        "        labels.append(label)\n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "X_sequences, y_sequences = split_into_sequences(dados_combinados)\n",
        "\n",
        "print(\"Dimensões das sequências de características:\", X_sequences.shape)\n",
        "print(\"Dimensões das sequências de rótulos:\", y_sequences.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensões dos dados de treinamento de características: (880, 10000, 5)\n",
            "Dimensões dos dados de teste de características: (220, 10000, 5)\n",
            "Dimensões dos dados de treinamento de rótulos: (880,)\n",
            "Dimensões dos dados de teste de rótulos: (220,)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sequences, y_sequences, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Dimensões dos dados de treinamento de características:\", X_train.shape)\n",
        "print(\"Dimensões dos dados de teste de características:\", X_test.shape)\n",
        "print(\"Dimensões dos dados de treinamento de rótulos:\", y_train.shape)\n",
        "print(\"Dimensões dos dados de teste de rótulos:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realização do treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "11/11 [==============================] - ETA: 0s - loss: nan - accuracy: 0.4801 \n",
            "Epoch 1: val_accuracy improved from -inf to 0.44886, saving model to model_checkpoint_ger_test.h5\n",
            "11/11 [==============================] - 200s 18s/step - loss: nan - accuracy: 0.4801 - val_loss: nan - val_accuracy: 0.4489\n",
            "Epoch 2/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Gabe\\anaconda3\\envs\\google-colab-ver\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - ETA: 0s - loss: nan - accuracy: 0.4801 \n",
            "Epoch 2: val_accuracy did not improve from 0.44886\n",
            "11/11 [==============================] - 205s 19s/step - loss: nan - accuracy: 0.4801 - val_loss: nan - val_accuracy: 0.4489\n",
            "Epoch 3/5\n",
            "11/11 [==============================] - ETA: 0s - loss: nan - accuracy: 0.4801 \n",
            "Epoch 3: val_accuracy did not improve from 0.44886\n",
            "11/11 [==============================] - 206s 19s/step - loss: nan - accuracy: 0.4801 - val_loss: nan - val_accuracy: 0.4489\n",
            "Epoch 4/5\n",
            "11/11 [==============================] - ETA: 0s - loss: nan - accuracy: 0.4801 \n",
            "Epoch 4: val_accuracy did not improve from 0.44886\n",
            "11/11 [==============================] - 206s 19s/step - loss: nan - accuracy: 0.4801 - val_loss: nan - val_accuracy: 0.4489\n",
            "Epoch 5/5\n",
            "11/11 [==============================] - ETA: 0s - loss: nan - accuracy: 0.4801 \n",
            "Epoch 5: val_accuracy did not improve from 0.44886\n",
            "11/11 [==============================] - 207s 19s/step - loss: nan - accuracy: 0.4801 - val_loss: nan - val_accuracy: 0.4489\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"model_checkpoint_ger_test.h5\"\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "model = Sequential([\n",
        "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "with h5py.File(checkpoint_path, 'r') as f:\n",
        "    best_model = tf.keras.models.load_model(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 7s 981ms/step - loss: nan - accuracy: 0.5045\n",
            "Acurácia do melhor modelo: 0.5045454502105713\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = best_model.evaluate(X_test, y_test)\n",
        "print(f'Acurácia do melhor modelo: {accuracy}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
